# Provider Configuration
PROVIDER_MODE=real  # Options: mock, real

# OpenAI API Configuration
# NOTE: Users provide their own API keys via X-OpenAI-API-Key request header
# The server does NOT store or use OPENAI_API_KEY
# See docs/API-KEY-PROXY-IMPLEMENTATION.md and deploy/API-KEY-SECURITY.md
# OPENAI_API_KEY=  # NEVER SET THIS - Users provide keys at request time
OPENAI_ORG_ID=  # Optional: Your OpenAI organization ID

# Model Configuration (Updated December 2025 - Cost-Optimized with GPT-5)
# See src/config/model-pricing.js for pricing details

# Operation-specific models for maximum cost optimization
# Using GPT-5 models for dramatic cost savings and superior quality
OPENAI_LLM_MODEL_EXPAND=gpt-5-nano    # Simple expansion: $0.05/1M tokens (3x cheaper than 4o-mini!)
OPENAI_LLM_MODEL_REFINE=gpt-5-mini    # Complex refinement: $0.25/1M tokens (best balance)
OPENAI_LLM_MODEL_COMBINE=gpt-5-nano   # Simple combining: $0.05/1M tokens (3x cheaper!)

# Fallback model (used if operation-specific not set)
OPENAI_LLM_MODEL=gpt-5-mini  # Default: $0.25/1M tokens

OPENAI_IMAGE_MODEL=gpt-image-1-mini  # Cost-efficient GPT-5 image generation
OPENAI_VISION_MODEL=gpt-5-nano  # Vision-capable GPT-5 with Flex pricing: $0.025/1M tokens (2.5x cheaper!)

# API Behavior
OPENAI_MAX_RETRIES=3
OPENAI_TIMEOUT_MS=90000

# Timezone Configuration
# Used for session history timestamps and file organization
TZ=America/Los_Angeles

ENSEMBLE_SIZE=3

# Flux Model Configuration
FLUX_MODEL_SOURCE=local
FLUX_MODEL_PATH=services/checkpoints/flux-dev-fp8.safetensors

# Flux LoRA Configuration
FLUX_LORA_PATH=
FLUX_LORA_SCALE=

# Flux Encoders (for custom local models)
# Only used when FLUX_MODEL_SOURCE=local
FLUX_TEXT_ENCODER_PATH=services/encoders/clip_l.safetensors
FLUX_TEXT_ENCODER_2_PATH=services/encoders/model.safetensors
FLUX_VAE_PATH=services/encoders/ae.safetensors

# Image Ranking Configuration
RANKING_MODE=tournament  # Options: tournament (VLM), scoring (Vision AI)

# HuggingFace Authentication
HF_TOKEN=  # HuggingFace API token for downloading models

# Local LLM Configuration (llama-cpp-python for prompt refinement)
LLM_PORT=8003
LLM_MODEL_REPO=TheBloke/Mistral-7B-Instruct-v0.2-GGUF
LLM_MODEL_FILE=*Q4_K_M.gguf
LLM_MODEL_PATH=  # Optional: absolute path to local GGUF file (overrides repo download)
LLM_GPU_LAYERS=32  # Number of layers on GPU (-1 = all layers)
LLM_CONTEXT_SIZE=2048

# Local Vision-Language Model (VLM) for Image Comparison
# Used for pairwise image ranking in beam search
# Options: Qwen2.5-VL-7B-Instruct (recommended), LLaVA 1.6, LLaVA 1.5
VLM_PORT=8004
VLM_MODEL_REPO=unsloth/Qwen2.5-VL-7B-Instruct-GGUF
VLM_MODEL_FILE=*Q4_K_M.gguf
VLM_CLIP_FILE=*mmproj-F16.gguf
VLM_MODEL_PATH=  # Optional: absolute path to local VLM GGUF file (overrides repo)
VLM_CLIP_PATH=  # Optional: absolute path to local CLIP projector file
VLM_GPU_LAYERS=-1  # -1 = all layers on GPU, 0 = CPU only
VLM_CONTEXT_SIZE=4096

# Local Vision Service Configuration (CLIP for image-text alignment)
LOCAL_VISION_PORT=8002
CLIP_MODEL=openai/clip-vit-base-patch32

