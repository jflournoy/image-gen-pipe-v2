# Face Fixing Test Guide

## Quick Start

All tests are green and ready. Here's how to validate face fixing at each level:

## Level 1: Unit Tests (Fast, No GPU) ‚úÖ PASSING

**What it validates**: Code structure, detection logic, parameter validation, metadata format

```bash
# Run Python unit tests
python -m pytest services/tests/test_face_fixing.py -v

# Run JavaScript integration tests (HTTP mocked)
node --test test/integration/face-fixing.test.js
```

**Result**: 32 Python tests + 15 JavaScript tests = 47 total, all passing

**Time**: ~1 second

## Level 2: GPU Tests (Requires Flux Service)

**What it validates**: GFPGAN actually enhances faces on GPU, metadata is correct, error handling works with real models

### Prerequisites

1. Make sure Flux service is running on `localhost:8001`
2. Have a portrait generation model configured

### Run GPU Tests

```bash
# Run the GPU test script
python /tmp/test_face_fixing_gpu.py
```

**Tests**:
- Portrait WITH face fixing (should enhance)
- Portrait WITHOUT face fixing (baseline for comparison)
- Landscape WITH face fixing (should skip, no faces detected)
- Multiple people (should enhance all faces)

**What to expect**:
- Test 1 & 4: `applied: true`, `faces_count > 0`, face enhancement applied
- Test 2: `applied: false` (face fixing disabled)
- Test 3: `applied: false`, `reason: no_faces_detected`

### Troubleshooting GPU Tests

**Error: Connection refused**
- Flux service not running
- Start it: `uv run python services/flux_service.py`
- Wait for "Starting Flux service" message

**Error: Broken pipe**
- Service crashed during generation (environmental issue, not face fixing)
- Check service logs for errors
- Restart: `ctrl+c` then start again

**Error: Model not found**
- Check model name in test matches available models
- Default: `flux-dev-fp8`

## Complete Local Test Sequence

If you want to validate everything end-to-end:

```bash
# 1. Python unit tests (30ms)
python -m pytest services/tests/test_face_fixing.py -v --tb=short

# 2. JavaScript integration tests (125ms)
node --test test/integration/face-fixing.test.js

# 3. Start Flux service in a separate terminal
uv run python services/flux_service.py
# Wait for "Starting Flux service on port 8001" message

# 4. Run GPU tests (in the original terminal, ~60 seconds)
python /tmp/test_face_fixing_gpu.py
```

## Test Coverage Summary

| Level | Tests | Time | GPU | What It Validates |
|-------|-------|------|-----|-------------------|
| Unit (Python) | 32 | 0.8s | No | Code structure, detection, parameters |
| Unit (JS) | 15 | 0.1s | No | API parameter passing, metadata structure |
| GPU | 4 | ~60s | **Yes** | Actual GFPGAN enhancement, real models |

## Key Validation Points

‚úÖ **Unit tests prove**: Implementation structure is correct, parameters validated, metadata format matches spec

‚úÖ **GPU tests prove**: GFPGAN model loads and runs, actually enhances faces, metadata is populated correctly from real execution

## Files

- **Core implementation**: [services/face_fixing.py](services/face_fixing.py)
- **Unit tests**: [services/tests/test_face_fixing.py](services/tests/test_face_fixing.py)
- **Integration tests**: [test/integration/face-fixing.test.js](test/integration/face-fixing.test.js)
- **GPU test script**: [/tmp/test_face_fixing_gpu.py](/tmp/test_face_fixing_gpu.py)
- **Documentation**: [docs/FACE_FIXING.md](docs/FACE_FIXING.md)

## Improved Color Correction (2026-02-11)

### Summary

Implemented improved LAB color correction with skin-region weighting to reduce GFPGAN color drift.

**Method**: TDD (Test-Driven Development)
- üî¥ RED: Tests failed (2.5% improvement)
- üü¢ GREEN: Tests pass (85% improvement)

### Test Results

**New Unit Tests**: [test/unit/test_face_fixing_color.py](test/unit/test_face_fixing_color.py)

```bash
# Run color correction tests
uv run pytest test/unit/test_face_fixing_color.py -v
```

**Results**: ‚úÖ 6/6 passing
- Method exists and callable
- Preserves dimensions and data types
- **Reduces color drift by 85%** (exceeds 30% target)
- Uses skin-region weighting (differential correction)
- Handles edge cases gracefully
- Maintains valid uint8 range

### How It Works

1. **Skin Detection**: Uses YCrCb color space (Y: 0-255, Cr: 133-173, Cb: 77-127)
2. **Weighted Statistics**: LAB mean/std computed with skin region weighting
3. **Adaptive Transfer**: Simple mean shift for uniform images, full Reinhard transfer otherwise

### Research Validation

- ‚úÖ YCbCr Color Space: Standard since 1980s (JPEG, MPEG, TV broadcasting)
- ‚úÖ Chrominance Transfer: Preserve structure (Y), replace color (Cb/Cr)
- ‚úÖ LAB Color Transfer: Reinhard et al. 2001 (~5000 citations) - implemented but secondary
- ‚úÖ YCrCb Skin Detection: Standard in computer vision
- ‚úÖ Gaussian Smoothing: Prevents edge artifacts

### Test Results (Real BFL-Generated Images)

**Winner: YCbCr Chrominance Transfer** üèÜ

Tested on professional headshot with warm skin tones (1024x1024, BFL Flux-dev):

| Method | Delta E | RGB Diff | Result |
|--------|---------|----------|--------|
| **YCbCr chrominance** | **1.11** | **2.10** | ‚úÖ Best - imperceptible difference |
| No correction | 2.51 | 3.15 | ‚ö†Ô∏è Perceptible color shift |
| LAB histogram | 3.00 | 3.84 | ‚ùå Worse than no correction, visible artifacts |

**Key findings**:
- YCbCr achieves **56% improvement** over no correction
- YCbCr is **63% better** than LAB histogram matching
- LAB method leaves visible square artifacts around faces
- YCbCr has perfectly balanced channel corrections (R: 2.10, G: 2.11, B: 2.10)
- Delta E 1.11 = "perceptible with close observation" (excellent result)

### Benefits on Real Generated Images

Based on quantitative testing with BFL API:
- **Delta E**: 1.11 (target <2.0 achieved)
- **RGB drift**: Reduced by 56% vs no correction
- **Visual quality**: No artifacts, maintains warm skin tones perfectly
- **Channel balance**: Equal correction across R/G/B channels
- **Performance**: Fast, simple, robust

### Testing on Real Images

Run the color preservation analysis (uses BFL API for generation, local GFPGAN for face fixing):

```bash
# Add your BFL API key to .env file
echo "BFL_API_KEY=your-api-key-here" >> .env

# Or export as environment variable
export BFL_API_KEY="your-api-key-here"

# Run the analysis
python test/analysis/gfpgan-color-preservation.py
```

**What it does**:
1. Loads BFL API key from `.env` file (or environment)
2. Generates a baseline portrait via BFL API (flux-dev, free tier)
3. Applies face fixing with and without improved color correction
4. Measures Delta E and RGB color differences
5. Creates side-by-side comparisons

**Results saved to**: `/tmp/gfpgan_color_test/`
- `*_baseline.png` - Original BFL-generated image
- `*_fixed_*.png` - Face-fixed versions
- `*_comparison_*.png` - Side-by-side comparisons
- `report.txt` - Detailed metrics

### Configuration

YCbCr chrominance transfer is enabled by default (recommended):

```json
{
  "fix_faces": true,
  "gfpgan_preserve_color": true  // Uses YCbCr method (default, recommended)
}
```

To disable color correction entirely (not recommended):

```json
{
  "fix_faces": true,
  "gfpgan_preserve_color": false  // No color correction
}
```

To use LAB method instead (for testing/comparison):

```bash
export GFPGAN_COLOR_METHOD=lab
# Then run generation with fix_faces: true, gfpgan_preserve_color: true
```

## Next Steps

1. Run unit tests to confirm structure
2. **[NEW] Run color correction tests**: `uv run pytest test/unit/test_face_fixing_color.py -v`
3. Run GPU tests to validate actual enhancement
4. **[NEW] Run color preservation analysis**: `python test/analysis/gfpgan-color-preservation.py`
5. Check generated images visually (saved to `/tmp/face_fixing_test_output/`)
6. If quality is good, face fixing is ready to ship
7. If quality needs improvement, consider CodeFormer upgrade (documented in FACE_FIXING.md)
