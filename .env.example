# Provider Configuration
PROVIDER_MODE=mock  # Options: mock, real

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ORG_ID=  # Optional: Your OpenAI organization ID

# Model Configuration (Updated December 2025 - Cost-Optimized Defaults)
# See src/config/model-pricing.js for complete pricing information
#
# Cost comparison (per 1M input tokens):
# - gpt-5-nano: $0.05 (best for simple tasks: expand, combine)
# - gpt-5-mini: $0.25 (recommended for most tasks: refine, critique)
# - gpt-5.1: $1.25 (flagship reasoning model)
# - gpt-4o-mini: $0.15 (recommended for vision tasks)
# - gpt-4o: $2.50 (premium vision)
# - gpt-4: $30.00 (legacy, avoid if possible)
#
# LLM Model Configuration (Two approaches available):
#
# Approach 1: Single model for all LLM operations (simple)
OPENAI_LLM_MODEL=gpt-5-mini  # Used for all operations if specific models not set
#
# Approach 2: Operation-specific models for maximum cost optimization (RECOMMENDED)
# These override OPENAI_LLM_MODEL for specific operations
OPENAI_LLM_MODEL_EXPAND=gpt-5-nano   # Simple expansion: $0.05/1M (3x cheaper than 4o-mini!)
OPENAI_LLM_MODEL_REFINE=gpt-5-mini   # Complex refinement: $0.25/1M (best quality/cost)
OPENAI_LLM_MODEL_COMBINE=gpt-5-nano  # Simple combining: $0.05/1M (3x cheaper!)
#
# Cost savings with operation-specific GPT-5 models:
# - Per 10K token run: $0.0015 (vs $0.30 with GPT-4)
# - Annual (1000 runs): $1.50 (vs $300 with GPT-4) - 99.5% savings!
# - Using nano for expand/combine provides additional 5x savings on those operations

OPENAI_IMAGE_MODEL=gpt-image-1-mini  # GPT Image 1 Mini (cost-efficient, $0.011 per 1024x1024 medium image)
OPENAI_VISION_MODEL=gpt-4o-mini  # Best value for vision: $0.15/1M tokens

# API Behavior
OPENAI_MAX_RETRIES=3
OPENAI_TIMEOUT_MS=30000

# Flux LoRA Configuration (Optional)
# Leave empty to use Flux without LoRA
FLUX_LORA_PATH=  # Path to LoRA weights (e.g., services/loras/flux-custom-lora.safetensors)
FLUX_LORA_SCALE=0.8  # LoRA strength (0.0-2.0, typically 0.7-1.0)

# Local Vision-Language Model (VLM) for Image Comparison
# Used for pairwise image ranking in beam search
# Recommended: Qwen2.5-VL-7B-Instruct (pure vision-language)
# Alternatives: LLaVA 1.6, LLaVA 1.5 (see services/README.md for details)
VLM_MODEL_REPO=unsloth/Qwen2.5-VL-7B-Instruct-GGUF
VLM_MODEL_FILE=*Q4_K_M.gguf
VLM_CLIP_FILE=*mmproj-F16.gguf
VLM_GPU_LAYERS=-1  # -1 = all layers on GPU, 0 = CPU only
VLM_CONTEXT_SIZE=4096
