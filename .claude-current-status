# Claude Status - Image URL Fix for Lineage Display
Last Updated: 2026-02-05 11:15:00

## Current Task: Fix Lineage Image URLs ‚úÖ

### Problem
Images generated after local LLM render weren't displaying in lineage view.
- Broken URLs: `http://localhost:3000/api/demo/images/2026-02-06/ses-203901/iter0-cand0.png`
- Working URLs: `http://localhost:3000/api/images/ses-203901/iter1-cand0.png`

### Root Cause
File: `public/demo.js:2023`

The fallback URL format in `buildLineageVisualization()` was wrong:
```javascript
// ‚ùå WRONG - includes date path
const imageUrl = step.imageUrl || `/api/demo/images/${jobData.date}/${jobData.sessionId}/...`;

// ‚úÖ CORRECT - endpoint auto-searches all dates
const imageUrl = step.imageUrl || `/api/images/${jobData.sessionId}/...`;
```

### Verification
The API endpoints in `src/api/demo-routes.js`:
- Line 225: `router.get('/images/:sessionId/:filename')` - searches all date directories
- Line 458: `router.get('/images/:date/:sessionId/:filename')` - for explicit dates

The beam-search-worker.js (line 322) constructs URLs using the first format:
```javascript
imageUrl = `/api/images/${sessionId}/${filename}`;
```

### Fix Applied
Changed line 2023 in `public/demo.js` to use correct URL format.

### Testing
Created test in `test/demo-lineage.test.js`:
- "should use correct image URL format in lineage fallback"
- Verifies fallback matches `/api/images/{sessionId}/{filename}` pattern
- All 8 lineage tests passing ‚úÖ

---

## Previous Task: VLM Crash Investigation & Fix ‚úÖ

### Problem Summary
VLM crashes during second ranking (iteration 1) after Flux‚ÜíVLM‚ÜíLLM‚ÜíFlux‚ÜíVLM cycle:
```
[10:37:07] üîÑ Ranking: Comparing i0c1 vs i1c0 (1/3) (failed: socket hang up)
[10:37:07] üîÑ Ranking: Comparing i0c1 vs i1c1 (2/3) (failed: VLM service unavailable)
[10:37:07] üîÑ Ranking: Comparing i1c0 vs i1c1 (3/3) (failed: VLM service unavailable)
```

### Root Cause Analysis

From `/tmp/beam-search-services/vlm.log`:
```
GGML_ASSERT((char *)addr + ggml_backend_buffer_get_alloc_size(buffer, tensor) <=
            (char *)ggml_backend_buffer_get_base(buffer) + ggml_backend_buffer_get_size(buffer)) failed
```

**Crash sequence:**
1. First VLM ranking (iteration 0) completes successfully
2. VLM unloaded, LLM runs, Flux runs
3. VLM reloads for second ranking
4. **CRASH on 7th image slice encoding** during first comparison
5. VLM Python process dies (socket hang up)
6. Subsequent requests find service unavailable

### Bugs Found

1. **`INFERENCE_RELOAD_THRESHOLD` defined but never checked** (line 55)
   - Counter increments but nothing triggers preventive reload
   - Safety net was disabled!

2. **`chat_handler` not properly closed on unload**
   - Vision encoder buffers may persist after unload
   - Just set to None, not explicitly closed

3. **`inference_count` not reset on unload**
   - Stale count persists across model reload cycles

### Fixes Applied

**File: `services/vlm_service.py`**

1. **Added preventive reload logic** (lines 335-359)
   ```python
   if inference_count >= INFERENCE_RELOAD_THRESHOLD:
       # Force full model reload to reset GGML buffers
       llm.close()
       load_model()
       inference_count = 0
   ```

2. **Proper chat_handler cleanup on unload** (lines 420-430)
   ```python
   if chat_handler is not None:
       if hasattr(chat_handler, 'close'):
           chat_handler.close()
       chat_handler = None
   ```

3. **Reset inference_count on unload** (line 436)
   ```python
   inference_count = 0
   ```

4. **Lowered threshold from 20 to 10** (line 55)
   - More conservative to prevent crashes in Flux‚ÜîVLM swap cycles

### Why This Should Work

- **Preventive reload**: After 10 inferences, model fully reloads with fresh GGML buffers
- **Clean unload**: Vision encoder buffers properly released before Flux loads
- **Fresh start**: Counter reset ensures no stale fragmentation state

### Testing Recommendation

Run the existing integration test which validates this exact scenario:
```bash
VLM_GPU_LAYERS=16 VLM_CONTEXT_SIZE=4096 node --test test/integration/vlm-production-settings.test.js
```

### Additional Tuning Options

If crashes persist, try:
1. Lower `VLM_GPU_LAYERS=8` (fewer layers = less GPU memory pressure)
2. Increase `GPU_CLEANUP_DELAY_MS=10000` (more time for CUDA cleanup)
3. Lower `VLM_INFERENCE_RELOAD_THRESHOLD=5` (more frequent reloads)

## Previous Session: Test Suite Improvements

Test Progress: 42 ‚Üí 28 failures (14 fixed)
Pass Rate: 884/912 passing (96.9%)
All Non-Intentional Failures: ‚úÖ Fixed
