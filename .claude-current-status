# Claude Current Status

## Session: 2026-01-29 16:00 â€” GPU Ensemble Voting & Full Pipeline Verification

### Test Fixes (Continuation from 2026-01-28)

Fixed 3 test failures in `test/integration/pipeline-vlm-llm.test.js`:
- Set `ensembleSize: 1` to match mock response count (avoid mock array exhaustion)
- Access `result.rankings` instead of `rankings` (rankImages returns `{rankings, metadata}`)
- Fixed undefined reference in pipeline flow verification

All 14 tests now passing (was 12/14). Commit: f062c75

### Critical Fixes Applied

**1. VLM Timeout Increase (Commit: 99a78fd)**
- Increased VLM comparison timeout: 60s â†’ 180s
- Allows VLM to load + run first inference after model swap
- File: `src/providers/local-vlm-provider.js:77`
- Env var: `VLM_TIMEOUT_MS` (default now 180000)

**2. GPU Model Cleanup Between Tests (Commit: ec9a3fb)**
- Added `afterEach` cleanup calling `modelCoordinator.cleanupAll()`
- Prevents GPU memory accumulation across test sequence
- Root cause: VLM stayed loaded (10.7GB) after pipeline test â†’ OOM on next test
- Added `KEEP_TEST_IMAGES=1` env var to preserve generated images

**3. Re-enabled Full Pipeline Test**
- Changed `it.skip()` to `it()` for Fluxâ†’VLMâ†’Fluxâ†’VLM test
- Updated test name to reflect 180s timeout fix

### Final GPU Integration Test Results (2026-01-29 20:00)

Ran full GPU test suite with all fixes (25 minutes total):

**âœ… Test 1: Full Pipeline (Fluxâ†’VLMâ†’Fluxâ†’VLM) - PASSED** (16.7 min)
- Step 1: Flux generates 2 images âœ…
- Step 2: VLM ranks round 1 (i0c0 vs i0c1 â†’ ok) âœ…
- Step 3: Flux generates 1 new image âœ…
- Step 4: VLM ranks round 2 (i0c0 vs i1c0 â†’ ok) âœ…
- All model swaps successful with 180s timeout
- Images saved: `output/temp/flux_*.png` (3 files)

**âœ… Cleanup Between Tests - WORKING**
- GPU VRAM before Test 2: 4553 MB (clean state)
- All models properly unloaded between tests
- No OOM crash during test sequence

**âœ… Test 2: Ensemble with Flux (ensembleSize=3) - PASSED** (8.5 min)
- VLM ensemble ranking: 3x sequential comparisons âœ…
- Winner: "ens0 â€” Rank 1 with 1/1 wins (3x ensemble voting)"
- GPU VRAM peak: 10069 MB (no crash with optimizations!)
- Images saved: `output/temp/flux_*.png` (2 files)

**âœ… Test 3: VLM-only Ensemble - PASSED** (26 sec)
- VLM ensemble with static images (ensembleSize=3) âœ…
- Winner: "static0 â€” Rank 1 with 1/1 wins (3x ensemble voting)"
- GPU VRAM peak: 11548 MB (stable, no crash!)
- Images saved: `/tmp/vlm-test-*.png` (2 files)

**Final Score: 11/11 tests passed (100%)** ðŸŽ‰

### VLM Memory Optimization Results (Commit: a54513e)

**Optimizations Applied:**
- GPU layers: -1 (all) â†’ 24 (partial offload)
- Context size: 4096 â†’ 2048 tokens
- Inference reload threshold: 50 â†’ 20

**Memory Measurements:**
- VLM loading: ~7GB (was ~10GB)
- VLM ensemble peak: ~11.5GB (with fragmentation during 3x voting)
- **Savings: ~3GB base, prevents OOM crashes**

**Test Results After Optimization (2026-01-29 21:30):**
- All GPU integration tests: **11/11 PASSED (100%)**
- Total duration: 24.3 minutes
- No crashes during ensemble voting âœ…
- VLM remained stable even at 11.5GB peak usage

### Key Achievements

âœ… **Full sequential Fluxâ†’VLMâ†’Fluxâ†’VLM pipeline works**
âœ… **180s VLM timeout handles model swap delays**
âœ… **GPU cleanup prevents OOM between tests**
âœ… **Image preservation for inspection (KEEP_TEST_IMAGES=1)**
âœ… **Sequential test execution (--test-concurrency=1) prevents parallel RAM spikes**

### Architecture Notes

**Flux Sequential CPU Offload (12GB GPU)**:
- Flux loads model into CPU RAM (34.5GB)
- During generation, moves each layer to GPU temporarily
- Prevents OOM on 12GB GPUs
- Expected behavior, not a bug

## Session: 2026-01-28 21:28 â€” Fix VLM Ensemble Crash

Root cause: VLM service crashed during ensemble voting (multiple sequential
`/compare` calls). `llm.reset()` is destructive and can crash llama-cpp-python
during rapid sequential inference. Also missing GC/CUDA cleanup between calls.

### Fixes Applied

| Fix | File | Description |
|-----|------|-------------|
| Prefer `kv_cache_clear()` | `vlm_service.py` | Use safe cache clear instead of destructive `reset()` |
| Post-inference GC | `vlm_service.py` | `gc.collect()` + `torch.cuda.empty_cache()` after each inference |
| Inference count tracking | `vlm_service.py` | Track sequential inference count (safety valve for reload) |
| `/compare-batch` endpoint | `vlm_service.py` | Batch comparisons in one HTTP call for ensemble voting |
| Error visibility | `local-vlm-provider.js` | `errorMessage` in progress callbacks |
| Ensemble reason string | `local-vlm-provider.js` | Shows `X/total wins (Nx ensemble voting)` |
| Error detail in UI | `beam-search.js` | Shows failure reason in progress messages |
| Flux timeout | `flux-image-provider.js` | 5min â†’ 10min for model reload after GPU swap |

### Tests Added

- `test/services/test_vlm_sequential_inference.py` â€” 7 tests for sequential inference stability
- `test/integration/vlm-ranking-pipeline.test.js` â€” 10 tests:
  - 8 mock tests (error visibility, ensemble info, happy path)
  - 1 GPU test: full VLMâ†’Fluxâ†’VLM cycle (requires model swaps)
  - 1 GPU test: VLM-only ensemble with static images (safer, no model swaps)

### GPU Stability Improvements

- `scripts/gpu-setup.sh` â€” Check/enable NVIDIA persistence mode to prevent driver crashes
- GPU VRAM monitoring in tests â€” Log memory usage before/after operations
- npm scripts: `gpu:setup`, `gpu:status`, `gpu:monitor`, `test:gpu`

**Root cause of desktop crash**: **Parallel test execution**. Node.js test runner defaults
to running tests in parallel (20 concurrent on 20-core system). Both Flux tests started
simultaneously, each loading 20GB model â†’ 40GB combined RAM spike â†’ OOM â†’ crash.
Fix: `--test-concurrency=1` forces sequential execution.

**Secondary issue**: NVIDIA persistence mode was disabled. When disabled, the driver can
be unloaded/reloaded during heavy GPU cycling, which can crash X server. Enable with
`sudo nvidia-smi -pm 1`.

### Open Issues

- #29: Test VLM resilience fixes with beam search
- #30: Wire UI settings to backend
- #31: VLM ensemble voting (partially addressed â€” need GPU verification)

## Previous Session: 2026-01-27 â€” VLM Stability Fixes

### Summary

Added VLM crash resilience across two failure modes:

| Fix | Description |
|-----|-------------|
| Request serialization | `asyncio.Lock` prevents concurrent VLM requests (llama_cpp thread-safety) |
| GPU cleanup delay | Increased to 5000ms for Flux -> VLM transitions |
| Load retry | Exponential backoff (3 attempts) for model load failures |
| KV cache clearing | `reset_kv_cache()` before each inference to prevent fragmentation |
| Inference retry | `run_inference_with_retry()` â€” unload/reload on mid-inference crash |

### Configuration

| Env Variable | Default | Description |
|-------------|---------|-------------|
| `GPU_CLEANUP_DELAY_MS` | 5000 | Wait after model unload before next load |
| `VLM_LOAD_RETRIES` | 3 | Max load attempts |
| `VLM_RETRY_DELAY` | 2.0 | Base delay (s) for load retry backoff |
| `VLM_INFERENCE_RETRIES` | 2 | Max inference retry attempts |

### Files Changed

- `services/vlm_service.py` â€” lock, retry, cache clearing
- `src/utils/model-coordinator.js` â€” GPU cleanup delay
- `test/services/test_vlm_service_concurrency.py` (new) â€” concurrency tests
