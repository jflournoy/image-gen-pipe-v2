# Claude Current Status

## Session: 2026-01-29 16:00 — GPU Ensemble Voting Verification

### Test Fixes (Continuation from 2026-01-28)

Fixed 3 test failures in `test/integration/pipeline-vlm-llm.test.js`:
- Set `ensembleSize: 1` to match mock response count (avoid mock array exhaustion)
- Access `result.rankings` instead of `rankings` (rankImages returns `{rankings, metadata}`)
- Fixed undefined reference in pipeline flow verification

All 14 tests now passing (was 12/14). Commit: f062c75

### GPU Ensemble Voting Test Results

Ran GPU integration tests with `ENABLE_GPU_TESTS=1 ENABLE_FLUX_TESTS=1`:

**✅ Success: VLM Ensemble Voting Works!**
- Test: "should complete ensemble voting with pre-existing images (VLM-only, no Flux reload)"
- **PASSED** - 3x ensemble voting completed successfully without VLM crash
- Ranking shows: "Rank 1 with 1/1 wins (3x ensemble voting)"
- All 3 sequential `/compare` calls succeeded

**❌ Timeout: Flux→VLM Model Swap**
- Test: "should complete ensemble voting (ensembleSize=3) without VLM crash (uses Flux once)"
- **FAILED** - VLM comparison timed out after 60 seconds
- Root cause: VLM auto-loads on first comparison request (model-coordinator.js:264)
  - VLM model load: ~60s
  - First inference: ~20-30s
  - **Total: 80-90s** vs 60s timeout
- Fix: Increased VLM comparison timeout from 60s → 180s (3 minutes)
  - File: `src/providers/local-vlm-provider.js:77`
  - Env var: `VLM_TIMEOUT_MS` (default now 180000)

## Session: 2026-01-28 21:28 — Fix VLM Ensemble Crash

Root cause: VLM service crashed during ensemble voting (multiple sequential
`/compare` calls). `llm.reset()` is destructive and can crash llama-cpp-python
during rapid sequential inference. Also missing GC/CUDA cleanup between calls.

### Fixes Applied

| Fix | File | Description |
|-----|------|-------------|
| Prefer `kv_cache_clear()` | `vlm_service.py` | Use safe cache clear instead of destructive `reset()` |
| Post-inference GC | `vlm_service.py` | `gc.collect()` + `torch.cuda.empty_cache()` after each inference |
| Inference count tracking | `vlm_service.py` | Track sequential inference count (safety valve for reload) |
| `/compare-batch` endpoint | `vlm_service.py` | Batch comparisons in one HTTP call for ensemble voting |
| Error visibility | `local-vlm-provider.js` | `errorMessage` in progress callbacks |
| Ensemble reason string | `local-vlm-provider.js` | Shows `X/total wins (Nx ensemble voting)` |
| Error detail in UI | `beam-search.js` | Shows failure reason in progress messages |
| Flux timeout | `flux-image-provider.js` | 5min → 10min for model reload after GPU swap |

### Tests Added

- `test/services/test_vlm_sequential_inference.py` — 7 tests for sequential inference stability
- `test/integration/vlm-ranking-pipeline.test.js` — 10 tests:
  - 8 mock tests (error visibility, ensemble info, happy path)
  - 1 GPU test: full VLM→Flux→VLM cycle (requires model swaps)
  - 1 GPU test: VLM-only ensemble with static images (safer, no model swaps)

### GPU Stability Improvements

- `scripts/gpu-setup.sh` — Check/enable NVIDIA persistence mode to prevent driver crashes
- GPU VRAM monitoring in tests — Log memory usage before/after operations
- npm scripts: `gpu:setup`, `gpu:status`, `gpu:monitor`, `test:gpu`

**Root cause of desktop crash**: **Parallel test execution**. Node.js test runner defaults
to running tests in parallel (20 concurrent on 20-core system). Both Flux tests started
simultaneously, each loading 20GB model → 40GB combined RAM spike → OOM → crash.
Fix: `--test-concurrency=1` forces sequential execution.

**Secondary issue**: NVIDIA persistence mode was disabled. When disabled, the driver can
be unloaded/reloaded during heavy GPU cycling, which can crash X server. Enable with
`sudo nvidia-smi -pm 1`.

### Open Issues

- #29: Test VLM resilience fixes with beam search
- #30: Wire UI settings to backend
- #31: VLM ensemble voting (partially addressed — need GPU verification)

## Previous Session: 2026-01-27 — VLM Stability Fixes

### Summary

Added VLM crash resilience across two failure modes:

| Fix | Description |
|-----|-------------|
| Request serialization | `asyncio.Lock` prevents concurrent VLM requests (llama_cpp thread-safety) |
| GPU cleanup delay | Increased to 5000ms for Flux -> VLM transitions |
| Load retry | Exponential backoff (3 attempts) for model load failures |
| KV cache clearing | `reset_kv_cache()` before each inference to prevent fragmentation |
| Inference retry | `run_inference_with_retry()` — unload/reload on mid-inference crash |

### Configuration

| Env Variable | Default | Description |
|-------------|---------|-------------|
| `GPU_CLEANUP_DELAY_MS` | 5000 | Wait after model unload before next load |
| `VLM_LOAD_RETRIES` | 3 | Max load attempts |
| `VLM_RETRY_DELAY` | 2.0 | Base delay (s) for load retry backoff |
| `VLM_INFERENCE_RETRIES` | 2 | Max inference retry attempts |

### Files Changed

- `services/vlm_service.py` — lock, retry, cache clearing
- `src/utils/model-coordinator.js` — GPU cleanup delay
- `test/services/test_vlm_service_concurrency.py` (new) — concurrency tests
