# Claude Current Status

## Session: 2026-01-29 16:00 — GPU Ensemble Voting & Full Pipeline Verification

### Test Fixes (Continuation from 2026-01-28)

Fixed 3 test failures in `test/integration/pipeline-vlm-llm.test.js`:
- Set `ensembleSize: 1` to match mock response count (avoid mock array exhaustion)
- Access `result.rankings` instead of `rankings` (rankImages returns `{rankings, metadata}`)
- Fixed undefined reference in pipeline flow verification

All 14 tests now passing (was 12/14). Commit: f062c75

### Critical Fixes Applied

**1. VLM Timeout Increase (Commit: 99a78fd)**
- Increased VLM comparison timeout: 60s → 180s
- Allows VLM to load + run first inference after model swap
- File: `src/providers/local-vlm-provider.js:77`
- Env var: `VLM_TIMEOUT_MS` (default now 180000)

**2. GPU Model Cleanup Between Tests (Commit: ec9a3fb)**
- Added `afterEach` cleanup calling `modelCoordinator.cleanupAll()`
- Prevents GPU memory accumulation across test sequence
- Root cause: VLM stayed loaded (10.7GB) after pipeline test → OOM on next test
- Added `KEEP_TEST_IMAGES=1` env var to preserve generated images

**3. Re-enabled Full Pipeline Test**
- Changed `it.skip()` to `it()` for Flux→VLM→Flux→VLM test
- Updated test name to reflect 180s timeout fix

### Final GPU Integration Test Results (2026-01-29 20:00)

Ran full GPU test suite with all fixes (25 minutes total):

**✅ Test 1: Full Pipeline (Flux→VLM→Flux→VLM) - PASSED** (16.7 min)
- Step 1: Flux generates 2 images ✅
- Step 2: VLM ranks round 1 (i0c0 vs i0c1 → ok) ✅
- Step 3: Flux generates 1 new image ✅
- Step 4: VLM ranks round 2 (i0c0 vs i1c0 → ok) ✅
- All model swaps successful with 180s timeout
- Images saved: `output/temp/flux_*.png` (3 files)

**✅ Cleanup Between Tests - WORKING**
- GPU VRAM before Test 2: 4553 MB (clean state)
- All models properly unloaded between tests
- No OOM crash during test sequence

**❌ Test 3: VLM-only Ensemble - FAILED**
- VLM service crashed with "socket hang up"
- VLM became unresponsive after extended testing
- Images saved: `/tmp/vlm-test-*.png` (2 files)
- Separate issue from pipeline testing (needs investigation)

**Final Score: 10/11 tests passed (91%)**

### Key Achievements

✅ **Full sequential Flux→VLM→Flux→VLM pipeline works**
✅ **180s VLM timeout handles model swap delays**
✅ **GPU cleanup prevents OOM between tests**
✅ **Image preservation for inspection (KEEP_TEST_IMAGES=1)**
✅ **Sequential test execution (--test-concurrency=1) prevents parallel RAM spikes**

### Architecture Notes

**Flux Sequential CPU Offload (12GB GPU)**:
- Flux loads model into CPU RAM (34.5GB)
- During generation, moves each layer to GPU temporarily
- Prevents OOM on 12GB GPUs
- Expected behavior, not a bug

## Session: 2026-01-28 21:28 — Fix VLM Ensemble Crash

Root cause: VLM service crashed during ensemble voting (multiple sequential
`/compare` calls). `llm.reset()` is destructive and can crash llama-cpp-python
during rapid sequential inference. Also missing GC/CUDA cleanup between calls.

### Fixes Applied

| Fix | File | Description |
|-----|------|-------------|
| Prefer `kv_cache_clear()` | `vlm_service.py` | Use safe cache clear instead of destructive `reset()` |
| Post-inference GC | `vlm_service.py` | `gc.collect()` + `torch.cuda.empty_cache()` after each inference |
| Inference count tracking | `vlm_service.py` | Track sequential inference count (safety valve for reload) |
| `/compare-batch` endpoint | `vlm_service.py` | Batch comparisons in one HTTP call for ensemble voting |
| Error visibility | `local-vlm-provider.js` | `errorMessage` in progress callbacks |
| Ensemble reason string | `local-vlm-provider.js` | Shows `X/total wins (Nx ensemble voting)` |
| Error detail in UI | `beam-search.js` | Shows failure reason in progress messages |
| Flux timeout | `flux-image-provider.js` | 5min → 10min for model reload after GPU swap |

### Tests Added

- `test/services/test_vlm_sequential_inference.py` — 7 tests for sequential inference stability
- `test/integration/vlm-ranking-pipeline.test.js` — 10 tests:
  - 8 mock tests (error visibility, ensemble info, happy path)
  - 1 GPU test: full VLM→Flux→VLM cycle (requires model swaps)
  - 1 GPU test: VLM-only ensemble with static images (safer, no model swaps)

### GPU Stability Improvements

- `scripts/gpu-setup.sh` — Check/enable NVIDIA persistence mode to prevent driver crashes
- GPU VRAM monitoring in tests — Log memory usage before/after operations
- npm scripts: `gpu:setup`, `gpu:status`, `gpu:monitor`, `test:gpu`

**Root cause of desktop crash**: **Parallel test execution**. Node.js test runner defaults
to running tests in parallel (20 concurrent on 20-core system). Both Flux tests started
simultaneously, each loading 20GB model → 40GB combined RAM spike → OOM → crash.
Fix: `--test-concurrency=1` forces sequential execution.

**Secondary issue**: NVIDIA persistence mode was disabled. When disabled, the driver can
be unloaded/reloaded during heavy GPU cycling, which can crash X server. Enable with
`sudo nvidia-smi -pm 1`.

### Open Issues

- #29: Test VLM resilience fixes with beam search
- #30: Wire UI settings to backend
- #31: VLM ensemble voting (partially addressed — need GPU verification)

## Previous Session: 2026-01-27 — VLM Stability Fixes

### Summary

Added VLM crash resilience across two failure modes:

| Fix | Description |
|-----|-------------|
| Request serialization | `asyncio.Lock` prevents concurrent VLM requests (llama_cpp thread-safety) |
| GPU cleanup delay | Increased to 5000ms for Flux -> VLM transitions |
| Load retry | Exponential backoff (3 attempts) for model load failures |
| KV cache clearing | `reset_kv_cache()` before each inference to prevent fragmentation |
| Inference retry | `run_inference_with_retry()` — unload/reload on mid-inference crash |

### Configuration

| Env Variable | Default | Description |
|-------------|---------|-------------|
| `GPU_CLEANUP_DELAY_MS` | 5000 | Wait after model unload before next load |
| `VLM_LOAD_RETRIES` | 3 | Max load attempts |
| `VLM_RETRY_DELAY` | 2.0 | Base delay (s) for load retry backoff |
| `VLM_INFERENCE_RETRIES` | 2 | Max inference retry attempts |

### Files Changed

- `services/vlm_service.py` — lock, retry, cache clearing
- `src/utils/model-coordinator.js` — GPU cleanup delay
- `test/services/test_vlm_service_concurrency.py` (new) — concurrency tests
